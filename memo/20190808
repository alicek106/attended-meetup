[1] 이웅원 님. 당근마켓. 딥러닝으로 동네생활 게시글 필터링하기. BERT

동네생활과 BERT. -> [동네생활 서비스]는 지역 정보 플랫폼의 역할. 동네에 관련된 이야기를 적을 수 있도록.
-> 안 좋은 거래 경험이나 업체 홍보 등을 올리는 사람들도 있다. 이걸 필터링 해야함.
- BERT : 2018년 구글이 발표한 모델. 자연어처리 (NLP) 
BERT의 핵심 : 텍스트 데이터는 엄청나게 많다. 사전 학습을 다양한 데이터에 대해서 먼저 하고. Pre-train -> Fine Tuning 하면 성능이 잘나옴. 
SQUAD : 질문을 던지면 모델이 답변을 하는 문제. (BERT가 거의 1등), KORQUAD도 BERT가 상위권. 

동네 생활과 BERT : 일반 분류 모델처럼 안하고 / 중고 거래 데이터로 사전 학습을 한 다음 (BERT) -> 동네 생활 데이터로 Fine Tuning?
동네 생활 데이터는 매우 적어서 딥러닝 하기가 힘듬. 근데 중고 거래 게시글은 한달에 몇백만이니, 중고 거래 게시글을 활용하는 식. -> 그걸 다시 동네 생활 데이터에 활용

BERT 간단 소개 : 
1. 문맥을 이해한 상태에서 학습을 하겠다는 뜻. Next Sentence Prediction : 다음 문장이 이게 맞냐, 를 맞추는 것.
-> 두 개의 문장을 줬을 때, 이어지는 문장인지 아닌지를 판별.
2. Masked Language Model : 문장을 다 쪼개 놨으면, 하나나 여러 개의 단어를 마스크로 가림. 가려진 부분이 원래 어땠는지를 맞추는 것.

중요한 것 : 데이터 셋의 사이즈. 
BERT Pretrain : 파이토치를 활용. 게시글 필터링 모델은 실시간성이 중요하지 않으므로, 비동기적으로 뒤에서 돌아간다. 편한 프레임웤 쓰면 됨.

중고거래 글 예시 : 맞춤법도 틀리고, 가격 사이즈 등의 숫자정보나 전화번호 URL이 있는데. 그런 식에서 마스킹을 하는 건 의미가 없음.
정규표현식 전처리 -> 전화번호나 URL 등을 특정 단어로 바꿔준다.
토크나이저 : wordpiece를 쓴다고 함 (알필요 없음). -> 언어에 한정되지 않는 형태소 분석 모델이라고 함 (구글?)
토크나이저 : sentencepiece라는게 있음. 알아서 토크나이징 해준다고 함.
결국은 sentencepiece는 안씀. 한국어에 대해서만 잘 되면 되기 때문에. 그래서 mecab 한국 형태소 분석기 사용 (3만개 voca). 모두 오픈소스임.

중고거래 게시글은 문장이 짧기도 해서 의미 파악이 어려움. 문장 개념이 모호함. 그래서 문장 단위로 자르고 두개의 덩어리로 씀.
전체 게시글을 두 개의 데이터를 보는 듯?

모델 학습 : 1주일 정도 학습. 근데 원래는 엄청 오래 학습해야 함. 20 에포크 학습. 
Next sentence : 93, Mask : 66%. 가려진 토큰을 맞추는것이 굉장히 정확하다고 말할 수 있음. 3만개나 되다 보니. 66%도 높은 편임.

동네생활 데이터는 8천개 정도인데, 라벨링을 2일에 걸쳐서 함.
덩네생활 데이터 필터링 모델은 엄청 빨리 끝남 10분?. 

배포 : Flask 씀. 선의의 피해자가 없도록 하는 것이 중요 (recall : 잘못 판단해서 필터링이 된 경우) 
예측 후처리. (???)
당연히 Prediction Model은 그냥 모델만 가져와서 씀. 그래서 Serving만 하도록.  
--
실시간성 중요하면 텐서플로우 쓰는게 좋음. AWS에서 쓰면 빠름.
다른 pretrain 한 모델을 가져다 쓸 수 있는가? -> 가능할것 같긴 함. 
LSTM, CNN, RNN을 해도 되긴 하는데, BERT가 더 좋게 성능이 나와서 BERT 씀.



----------- 절취선 -------------
Link : https://docs.google.com/presentation/d/1_v-f5B67v-hcmEbltLEfjSS5MKKctztdAGZzEHs2DPM/edit#slide=id.g5adadfee39_0_57
[2] 그들이 AWS에서 데이터 파이프라인을 구축하는 방법. 박훈 님. 야놀자.
이 발표는 정답이 아님. 회사마다 데이터가 다르니까 필요한 인프라가 다르다. 실시간이 필요하지 않을수도 있고, 머신러닝이 답이 아닐수도 있음.
AWS에서 데이터의 생산, 소비를 모두 빠르게 진행하는 방법 ? 에 대해서 다룬다. 비용 싸게, 가능하면 적은 인원으로 다룬다든지. 

야놀자는 어떤 데이터를 다루나? : 국내국외 숙박. 숙박 형태에 따라서 데이터의 구조가 매우 다르다. 계절 카테고리나 연박단박 등등.. 데이터의 분석 방법이 달라야 한다.
데이터 파이프라인의 순서 : 수집, 처리, 조회, 서비스. 

두 번째 이야기. AWS 위에서 데이터를 분석하는 방법.
스타트업에서는 그냥 직접 쿼리해도 되는데, 규모가 커지면 DB를 물리적으로 분할해야 함. DB를 조인하고 싶어지는데..
RDB뿐만 아니라 ES, ElasticCache 등의 수요도 생김. 

실시간에 가까워질수록 더 힘들다. 
목표 : 운영, 인프라 비용 최소화.  

클라이언트 로그 샘플 : 노출, 클릭, 뷰 3가지.

Kafka는 운영 비용이 높다. 모니터링도 해야하고. 주키퍼 클러스터도 필요하고. 앞단에서 받아줄 ELB, Nginx도 필요하니까.
-> 대안으로 키네시스로 감. 클라이언트 로그의 경우에는 키네시스. (Limitation : Connector 지원이 카프카에 비해 빈약함)

서버 로그 : WEB, WAS. Agent는 단순히 로그 파일로 하고. 라이브러리는 중요 이벤트를 전송한다.
EB의 경우에는 로그 파일을S3로. 그대신 인터벌 있음. 실시간이 필요하면 키네시스.

카프카는 IP나 포트로 ACL이 가능한데, 키네시스는 IAM으로 관리를 하다보니. 아마존 어카운트가 다르면 너무 어렵다. 그래서 대안으로 Managed Kafka도 있음.

EMR도 씀. 관리형 처리 프레임워크. 스파크, 플링크 그런 것들. 깔고 운영하려면 노동이 들어가는 것들이 설치되어 있음. 키네시스에서 쏜 것을 EMR에서 받아서..

스파크 스트리밍이 필요한 이유? 왜 스파크에서 쏘나요? -> 

시계열은 파티션 단위로 관리함. 일, 시간 단위로 자른다. 근데 늦게 들어온 데이터는? 어떻게 처리.? -> Cold, Hot으로 데이터를 나눈다. (무슨 뜻인지는 잘 모르겠다)

---
데이터 스토리지 수집 대상 : 데이터베이스 레디스, ES 등등. 
스토리지의 경우에는: 회사에 DB가 너무많다. 기존 DB 컬럼 변경 케이스도 따라가야 함. 데이터 컬럼 변경은 이메일로 개발자들에게 전송.
EMR을 쓴다. Digdag 스케줄러 사용. 전부 EMR로 데이터를 수집 (S3가 메인 저장소, Hive MetaStore 도 사용 : 스키마 저장소.)

Columnar vs Row-based ?  -> 그럼 컬럼을 묶어서 데이터를 저장하면 더 빠르지 않아? 라는 아이디어에서 가져옴. 빠르고 사이즈가 작다고 함.
실제 데이터는 S3에 저장. 파티션, 메타데이터는 Hive 메타데이터에.

---
AWS 로그 ? AWS 로그는 데이터 처리 분석 플랫폼이 있는 곳으로 정하는 것이 정신 건강에 좋음 (prod, data 계정을 따로 나누면 짜증남)

---
처리?
--- 조회? 조회 툴은 너무 많으니. 다양한 도구를 주되 범용 언어를 쓴다. SQL 사용. 데이터 조회는 Presto를 씀. 
Re:Dash 라는 툴이 있다고 함. 엄청 좋다는데 뭘 하는건진.?

//
AWS Call Limit이 존재해서, 많은 사람이 계정이 한꺼번에 사용하면 고려해야 함. 20~30명만 해도 바로 걸림. 그리고 잘 늘려주지도 않음.
Dev, Prod, Data 용도에 따라서 계정을 나누는게 좋음.
Assume Role? 다른 계정의 리소스를 사용할 수 있도록 다른 어카운트의 롤을 가장해서 쓰는 것.

// MSK는 스케일 업 아웃 버전 업그레이드 다 안됨. 주의.
